{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "639f3dfe",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd036a8d",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "9156ee3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import contractions\n",
    "from tqdm import tqdm\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from collections import Counter, defaultdict\n",
    "tqdm.pandas()\n",
    "\n",
    "# nltk.download(\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be66a007",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "0ea39c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Data/RawData.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7626f51",
   "metadata": {},
   "source": [
    "# Drop Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "bcb3784b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6283156e",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "f907bda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"AddData/IndoStopwords.txt\", encoding=\"utf-8\") as f:\n",
    "    BASE_STOPWORDS = set(f.read().split())\n",
    "EXTRA_STOPWORDS = {\"eh\", \"dn\", \"an\", \"yang\", \"suami\", \"anak\", \"rumah\", \"ya\", \"ga\", \"aja\", \"gak\", \"orang\", \"ku\", \"bun\", \"kerja\", \"istri\", \"uang\", \"kasih\", \"kalau\", \"orang tua\"}\n",
    "ALL_STOPWORDS = BASE_STOPWORDS|EXTRA_STOPWORDS\n",
    "\n",
    "def stop_word(word: str):\n",
    "    if word not in ALL_STOPWORDS:\n",
    "        return word "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e91b6c9",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "d691c939",
   "metadata": {},
   "outputs": [],
   "source": [
    "kamus_clean = []\n",
    "with open('AddData/IndoDict.txt') as f:\n",
    "    for line in f:\n",
    "        line = line.strip().lower()\n",
    "        word = line.split()[0]\n",
    "        kamus_clean.append(word)\n",
    "\n",
    "akar_kata = []\n",
    "def kamus_word(word):\n",
    "    if word in kamus_clean:\n",
    "        akar_kata.append(word)\n",
    "        return None\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "8293792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hapus_infleksional_suffiks(word):\n",
    "    rules = [\n",
    "        (('lah', 'kah', 'nya', 'tah', 'pun'), 3),\n",
    "        (('ku', 'mu'), 2),\n",
    "    ]\n",
    "\n",
    "    for suffixes, cut_len in rules:\n",
    "        if word.endswith(suffixes):\n",
    "            stem = word[:-cut_len]\n",
    "            return kamus_word(stem)\n",
    "\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "10ab42ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hapus_derivation_suffiks(word):\n",
    "    if word in kamus_clean:\n",
    "        return word\n",
    "\n",
    "    # urutan penting: 'kan' dulu, lalu 'an', lalu 'i'\n",
    "    for suf in ('kan', 'an', 'i'):\n",
    "        if word.endswith(suf):\n",
    "            stem = word[:-len(suf)]\n",
    "\n",
    "            # opsional: cegah stem yang terlalu pendek\n",
    "            if len(stem) < 3:\n",
    "                continue\n",
    "\n",
    "            # hanya terima kalau stem ada di kamus\n",
    "            if stem in kamus_clean:\n",
    "                return stem\n",
    "\n",
    "    # kalau tidak ada aturan yang cocok, balikin kata aslinya\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "eb039091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hapus_derivation_prefiks(word):\n",
    "    # kalau kata sudah kata dasar di kamus, jangan dipotong prefiks\n",
    "    if word in kamus_clean:\n",
    "        return word\n",
    "\n",
    "    def check(stem, restore=None):\n",
    "        # buang derivational suffix dulu\n",
    "        stem2 = hapus_derivation_suffiks(stem)\n",
    "\n",
    "        candidates = [stem2]\n",
    "        if restore is not None:\n",
    "            candidates.append(restore + stem2)\n",
    "\n",
    "        for cand in candidates:\n",
    "            if cand in kamus_clean:      # langsung cek ke kamus\n",
    "                return cand\n",
    "        return None\n",
    "\n",
    "    # (prefixes, cut_len, restore_consonant)\n",
    "    rules = [\n",
    "        (('mempel',),                  6, None),\n",
    "        (('memper',),                  6, None),\n",
    "        (('diper', 'keber', 'keter'),  5, None),\n",
    "        (('meng', 'peng'),             4, 'k'),  # meng-/peng- → pulihkan k\n",
    "        (('meny', 'peny'),             4, 's'),  # meny-/peny- → pulihkan s\n",
    "        (('mel', 'mer', 'pel', 'per'), 3, None),\n",
    "        (('men', 'pen'),               3, 't'),  # men-/pen- → pulihkan t\n",
    "        (('mem', 'pem'),               3, 'p'),  # mem-/pem- → pulihkan p\n",
    "        (('bel', 'ber', 'tel', 'ter'), 3, None),\n",
    "        (('di', 'ke', 'se'),           2, None),\n",
    "        (('be', 'te'),                 2, None),\n",
    "        (('me', 'pe'),                 2, None),\n",
    "    ]\n",
    "\n",
    "    for prefixes, cut_len, restore in rules:\n",
    "        if word.startswith(prefixes) and len(word) > cut_len:\n",
    "            sub_word = word[cut_len:]\n",
    "            result = check(sub_word, restore)\n",
    "            if result is not None:\n",
    "                return result\n",
    "\n",
    "    # kalau tidak ada aturan yang berhasil, balikin kata aslinya\n",
    "    return word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde41c55",
   "metadata": {},
   "source": [
    "## Cleaning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "739e389a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kamus_set = set(kamus_clean)\n",
    "UNKNOWN_MAP = defaultdict(Counter)\n",
    "\n",
    "key_df = pd.read_csv(\"AddData/IndoSlangMap.csv\")\n",
    "key_df[\"replace\"] = key_df[\"replace\"].fillna(\"\").astype(str)\n",
    "REPLACE_MAP = dict(zip(key_df[\"original_word\"], key_df[\"replace\"]))\n",
    "\n",
    "def clean(text: str) -> str:\n",
    "    text = str(text)\n",
    "\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # handle gara2/gara-gara dan yang sejenis\n",
    "    text = re.sub(r\"\\b([a-z]+)-\\1\\b\", r\"\\1\", text)\n",
    "    text = re.sub(r\"([a-zA-Z])2\", r\"\\1\", text)\n",
    "\n",
    "    # remove non-alphanumeric characters\n",
    "    text = re.sub(r\"[^a-z0-9\\s]+\", \" \", text)\n",
    "\n",
    "    # separate digits and letters\n",
    "    text = re.sub(r\"(\\d)([a-zA-Z])\", r\"\\1 \\2\", text)\n",
    "    text = re.sub(r\"([a-zA-Z])(\\d)\", r\"\\1 \\2\", text)\n",
    "\n",
    "    # normalize whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    for w in text.split():\n",
    "        w = REPLACE_MAP.get(w, w)\n",
    "\n",
    "        # 1) stopword\n",
    "        w = stop_word(w)\n",
    "        if not w:\n",
    "            continue\n",
    "        \n",
    "        original = w\n",
    "        \n",
    "        # 2) kalau sudah kata kamus, pakai apa adanya\n",
    "        if w in kamus_clean:\n",
    "            tokens.append(w)\n",
    "            continue\n",
    "\n",
    "        # 3) hapus infleksional suffiks\n",
    "        w = hapus_infleksional_suffiks(w)\n",
    "        if not w:\n",
    "            continue\n",
    "\n",
    "        # 4) hapus derivation suffiks\n",
    "        w = hapus_derivation_suffiks(w)\n",
    "        if not w:\n",
    "            continue\n",
    "\n",
    "        # 5) hapus derivation prefiks\n",
    "        w = hapus_derivation_prefiks(w)\n",
    "        if not w:\n",
    "            continue\n",
    "\n",
    "        # log kata yang masih bukan kata kamus\n",
    "        if w not in kamus_set:\n",
    "            UNKNOWN_MAP[w][original] += 1\n",
    "\n",
    "        tokens.append(w)\n",
    "\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "1697989a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2163/2163 [01:00<00:00, 35.90it/s]\n"
     ]
    }
   ],
   "source": [
    "data['text'] = data['text'].progress_apply(clean)\n",
    "data = data.dropna().reset_index(drop=True)\n",
    "data.to_csv(\"Data/CleanData.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "8798d6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for final_word, counter in UNKNOWN_MAP.items():\n",
    "    final_total = sum(counter.values())\n",
    "    for original_word, count in counter.most_common():\n",
    "        rows.append({\n",
    "            \"original_word\": original_word,\n",
    "            \"final_word\": final_word,\n",
    "            \"pair_count\": count,\n",
    "            \"final_total\": final_total,\n",
    "        })\n",
    "\n",
    "df_unknown_map = pd.DataFrame(rows).sort_values(\n",
    "    [\"final_total\", \"pair_count\"], ascending=False\n",
    ").reset_index(drop=True)\n",
    "\n",
    "df_unknown_map.to_csv(\"random_words_mapping.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ProjCompBio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
